<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="last-modified" content="2017-03-03 15:16:12 +0100">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- meta "search-domain" used for google site search function google_search() -->
    <meta name="search-domain" value="">
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="../assets/css/lesson.css" />
    
    <link rel="shortcut icon" type="image/x-icon" href="/favicon-swc.ico" />
    
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
	<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
	<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
	<![endif]-->
    <title>HPC in a day: Distributing computations among computers</title>
  </head>
  <body>
    <div class="container">
      
<nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      
      
      <a href="https://software-carpentry.org" class="pull-left">
        <img class="navbar-logo" src="../assets/img/swc-icon-blue.svg" alt="Software Carpentry logo" />
      </a>
      

      
      <a class="navbar-brand" href="../">Home</a>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

	
        <li><a href="../conduct/">Code of Conduct</a></li>

	
        
        <li><a href="../setup/">Setup</a></li>
        <li><a href="../reference/">Reference</a></li>
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Episodes <span class="caret"></span></a>
          <ul class="dropdown-menu">
            
            <li><a href="../02-01-batch-systems-101/">Batch systems and schedulers 101</a></li>
            
            <li><a href="../02-02-advanced-job-scheduling/">Working with the scheduler</a></li>
            
            <li><a href="../03-01-parallel-estimate-of-pi/">Parallel Estimation of Pi for Pedestrians</a></li>
            
            <li><a href="../03-02-mpi-for-pi/">Distributing computations among computers</a></li>
            
            <li><a href="../03-03-mapreduce-for-pi/">Searching for the answer to life, the universe and everything</a></li>
            
          </ul>
        </li>
	

	
	
        <li class="dropdown">
          <a href="../" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Extras <span class="caret"></span></a>
          <ul class="dropdown-menu">
            
          </ul>
        </li>
	

	
        <li><a href="../license/">License</a></li>
      </ul>
      <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form>
    </div>
  </div>
</nav>


<div class="row">
  <div class="col-md-1">
    <h3>
      
      <a href="../03-01-parallel-estimate-of-pi/"><span class="glyphicon glyphicon-menu-left" aria-hidden="true"></span><span class="sr-only">previous episode</span></a>
      
    </h3>
  </div>
  <div class="col-md-10">
    
    <h3 class="maintitle"><a href="../">HPC in a day</a></h3>
    <h1 class="maintitle">Distributing computations among computers</h1>
    
  </div>
  <div class="col-md-1">
    <h3>
      
      <a href="../03-03-mapreduce-for-pi/"><span class="glyphicon glyphicon-menu-right" aria-hidden="true"></span><span class="sr-only">next episode</span></a>
      
    </h3>
  </div>
</div>


<blockquote class="objectives">
  <h2>Overview</h2>

  <div class="row">
    <div class="col-md-3">
      <strong>Teaching:</strong> 45 min
      <br/>
      <strong>Exercises:</strong> 10 min
    </div>
    <div class="col-md-9">
      <strong>Questions</strong>
      <ul>
	
	<li><p>How do I exploit parallelism using the message passing interface (MPI)?</p>
</li>
	
      </ul>
    </div>
  </div>

  <div class="row">
    <div class="col-md-3">
    </div>
    <div class="col-md-9">
      <strong>Objectives</strong>
      <ul>
	
	<li><p>Explain how message passing allows performing computations in more than 1 computer at the same time.</p>
</li>
	
	<li><p>Observe the effects of parallel execution of commands with a simple hostname call.</p>
</li>
	
	<li><p>Measure the runtime of parallel and mpi version of the implementation.</p>
</li>
	
      </ul>
    </div>
  </div>

</blockquote>

<p>Lola Lazy is now confident enough to work with the batch system of the cluster. She now turns her attention to the problem at hand, i.e. estimating the value of <em>Pi</em> to very high precision.</p>

<p>One of her more experienced colleagues has suggested to her, to use the <em>Message Passing Interface</em> (in short: <em>MPI</em>) for that matter. As she has no prior knowledge in the field, accepting this advice is as good as trying some other technique on her how. She first explores the documentation of MPI a bit to get a feeling about the philosophy behind this approach.</p>

<blockquote>
  <h2 id="message-passing-interface">Message Passing Interface</h2>
  <p>A long time before we had smart phones, tablets or laptops, <a href="http://www.phy.duke.edu/~rgb/brahma/Resources/beowulf/papers/ICPP95/icpp95.html">compute clusters</a> were already around and consisted of interconnected computers that had merely enough memory to show the first two frames of a movie (<code class="highlighter-rouge">2*1920*1080*4 Bytes = 16 MB</code>). 
However, scientific problems back than were equally demanding more and more memory than today. 
To overcome the lack of available hardware memory, <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface#History">specialists from academia and industry</a> came about with the idea to consider the memory of several interconnected compute nodes as one. Given a standardized software that synchronizes the various states of memory between the client/slave nodes during the execution of driver application through the network interfaces. With this performing large calculations that required more memory than each individual cluster node can offer was possible. Moreover, this technique by passing messages (hence <em>Message Passing Interface</em> or <em>MPI</em>) on memory updates in a controlled fashion allowed to write parallel programs that were capable of running on a diverse set of cluster architectures.</p>
</blockquote>

<p><img src="../tikz/cluster_schematic.svg" alt="Schematic View of a Compute Cluster with 4 nodes (12 cores each)" /></p>

<p>Lola becomes curious. She wants to experiment with this parallelisation technique a bit. For this, she would like to print the name of the node where a specific driver application is run.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">$ bsub -n 4 -o call_hostname.out -e call_hostname.err mpirun hostname</code></pre></figure>

<p>The log file that is filled by the <code class="highlighter-rouge">bsub</code> command, contains the following lines after finishing the job:</p>

<div class="output highlighter-rouge"><pre class="highlight"><code>n01
n01
n01
n01
</code></pre>
</div>

<p>The output makes her wonder. Apparently, the command was cloned and executed on the same host 4 times. If she increases the number of processors to a number larger than the number of CPU cores each of here nodes has, this might change and the distributed nature of <code class="highlighter-rouge">mpirun</code> will reveal itself.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">$ bsub -n 16 -o call_hostname.out -e call_hostname.err mpirun hostname</code></pre></figure>

<div class="output highlighter-rouge"><pre class="highlight"><code>n01
n01
n01
n01
n01
n01
n01
n01
n01
n01
n01
n02
n01
n02
n02
n02
</code></pre>
</div>

<p><img src="../tikz/mpirunhostname_on_clusterschematic.svg" alt="Execution of `mpirun hostname` on a Compute Cluster with 4 nodes (12 cores each)" /></p>

<p>As the figure above shows, 12 instances of <code class="highlighter-rouge">hostname</code> were called on <code class="highlighter-rouge">n01</code> and 4 more on <code class="highlighter-rouge">n02</code>. Strange though, that the last 5 lines are not ordered correctly. Upon showing this result to her colleaque, the latter explains: even though, the <code class="highlighter-rouge">hostname</code> command is run in parallel across the 2 nodes that are used here, the output of her 16 <code class="highlighter-rouge">hostname</code> calls need to be merged into one output file (that she called <code class="highlighter-rouge">call_hostname.out</code>) at the end. This synchronization performed by the <code class="highlighter-rouge">mpirun</code> application is not guaranteed to happen in an ordered fashion (how could it be as the commands were issued in parallel). Her colleaque explains, that the <code class="highlighter-rouge">hostname</code> application itself is not aware of <em>MPI</em> in a way that it is not parallelized with it. Thus, the <code class="highlighter-rouge">mpirun</code> driver simply accesses the nodes that it is allowed to run on by the batch system and launches the <code class="highlighter-rouge">hostname</code> app. After that, <code class="highlighter-rouge">mpirun</code> collects the output of the executed commands at completion and writes it to the defined output file <code class="highlighter-rouge">call_hostname.out</code>.</p>

<p>Like a reflex, Lola asks how to write these MPI programs. Her colleague points out that she needs to program the languages that MPI supports, such as Fortran, C, C++, python and many more. As Lola is most confident with python, her colleague wants to give her a head start using <code class="highlighter-rouge">mpi4py</code> and provides a minimal example. This example is analogous to what Lola just played with. This python script called <code class="highlighter-rouge">print_hostname.py</code> prints the number of the current MPI rank (i.e. the unique id of the execution thread within one mpirun invocation), the total number of MPI ranks available and the hostname this rank is currently run on.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">$ bsub -n 4 -o call_hostname.out -e call_hostname.err mpirun python3 print_hostname.py</code></pre></figure>

<div class="output highlighter-rouge"><pre class="highlight"><code>this is 16/16 running on n02
this is 15/16 running on n02
this is 13/16 running on n02
this is 14/16 running on n02
this is  3/16 running on n01
this is  5/16 running on n01
this is 11/16 running on n01
this is  1/16 running on n01
this is  7/16 running on n01
this is  2/16 running on n01
this is  4/16 running on n01
this is  6/16 running on n01
this is  8/16 running on n01
this is  9/16 running on n01
this is 10/16 running on n01
this is 12/16 running on n01
</code></pre>
</div>

<p>Again, the unordered output is visible. Now, the relation between the rank and the parameters <code class="highlighter-rouge">-n</code> to submit command becomes more clear. <code class="highlighter-rouge">-n</code> defines how many processors the current invocation of mpirun requires. If <code class="highlighter-rouge">-n 16</code> is defined, the rank can run from <code class="highlighter-rouge">0</code> to <code class="highlighter-rouge">15</code>.</p>

<blockquote class="challenge">
  <h2 id="does-mpirun-really-execute-commands-in-parallel">Does <code class="highlighter-rouge">mpirun</code> really execute commands in parallel?</h2>

  <p>Launch the command <code class="highlighter-rouge">date</code> 16 times across your cluster. What do you observe? Play around with the precision of date through its flags (<code class="highlighter-rouge">+%N</code> for example) and study the distribution of the results.</p>

</blockquote>

<blockquote class="challenge">
  <h2 id="upgrade-printhostnamepy-and-print-the-time-of-day-as-well">Upgrade <code class="highlighter-rouge">print_hostname.py</code> and print the time-of-day as well</h2>

  <p>Open the <code class="highlighter-rouge">print_hostname.py</code> script with your editor and use the python3 <code class="highlighter-rouge">datetime</code> module to print the time of day next to the host name and rank number.</p>

</blockquote>

<p>To finalize this day’s work, Lola wants to tackle distributed memory parallelisation using the Message Passing Interface (MPI). For this, she uses the <code class="highlighter-rouge">mpi4py</code> library that is preinstalled on her cluster. She again starts from the <a href="code/03_parallel_jobs/serial_numpi.py">serial implementation</a>. At first, she expands the include statements a bit.</p>

<div class="python highlighter-rouge"><pre class="highlight"><code>from mpi4py import MPI

comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()
</code></pre>
</div>

<p>These 4 lines will be very instrumental through out the entire MPI program. The entire MPI software stack builds upon the notion of a communicator. Here, we see the MPI.COMM_WORLD communicator by which all processes that are created talk to each other. We will use it as a hub to initiate communications among all participating processes. Subsequently, we ask <code class="highlighter-rouge">comm</code> how many participants are connected by calling <code class="highlighter-rouge">comm.Get_size()</code>. Then we’ll ask the communicator, what rank the current process is <code class="highlighter-rouge">comm.Get_rank()</code>. And with this, Lola has entered the dungeon of MPI.</p>

<blockquote class="callout">
  <h2 id="every-line-is-running-in-parallel">Every Line Is Running in Parallel!</h2>
  <p>As discussed in the previous section, a call to <code class="highlighter-rouge">&lt;your scheduler&gt; mpirun &lt;your program&gt;</code> will do the following:
    - <code class="highlighter-rouge">mpirun</code> will obtain a list of available nodes from the scheduler
    - mpirun will then <code class="highlighter-rouge">ssh</code> onto these nodes for you and instantiate a local mpirun there
    - this local mpirun will execute <code class="highlighter-rouge">&lt;your program&gt;</code> in parallel to all the others and call every line of it from top to bottom
    - only if your program reaches a statement of the form <code class="highlighter-rouge">comm.do_something(...)</code>, your program will start communicating through the mpi library with the other mpi processes; this communication can entail point-to-point data transfers or collective data transfers (that’s why it’s called ‘message passing’ because MPI does nothing else than provide mechanism to send messages around the cluster), depending on the type of communication, the MPI library might make your program wait until the all message passing has been completed
In case you want to do something only on one rank specifically, you can do that by:</p>
  <div class="highlighter-rouge"><pre class="highlight"><code>if rank == 0:
    print("Hello World")
</code></pre>
  </div>
</blockquote>

<p>Pushing the implementation further, the list of <code class="highlighter-rouge">partitions</code> needs to be established similar to what was done in the parallel implementation above. Also a list for the results is created and all items are initialized to <code class="highlighter-rouge">0</code>.</p>

<div class="python highlighter-rouge"><pre class="highlight"><code>if rank == 0:
    partitions = [ int(n_samples/size) for item in range(size)]
    counts = [ int(0) ] *size
else:
    partitions = None
    counts = None
</code></pre>
</div>

<p>In this example, you can see how the lists are only created on one rank for now (rank <code class="highlighter-rouge">0</code> to be precise). At this, point the contents of <code class="highlighter-rouge">partitions</code> and <code class="highlighter-rouge">counts</code> reside on rank <code class="highlighter-rouge">0</code> only. They now have to send to all other participanting ranks.</p>

<div class="python highlighter-rouge"><pre class="highlight"><code>partition_item = comm.scatter(partitions, root=0)
count_item = comm.scatter(counts, root=0)
</code></pre>
</div>

<p>Note how the input variable is <code class="highlighter-rouge">partitions</code> (aka a list of values) and the output variable is named <code class="highlighter-rouge">partition_item</code>. This is because, <code class="highlighter-rouge">mpi4py</code> returns only one item (namely the one item in <code class="highlighter-rouge">partitions</code> matching the rank of the current process, i.e. <code class="highlighter-rouge">partitions[rank]</code>) rather than the full list. Now, the actual work can be done.</p>

<div class="python highlighter-rouge"><pre class="highlight"><code>count_item = inside_circle(partition_item)
</code></pre>
</div>

<p>This is the known function call from the serial implementation. After this, the results have to be communicated back again.</p>

<div class="python highlighter-rouge"><pre class="highlight"><code>counts = comm.gather(count_item, root=0)
</code></pre>
</div>

<p>The logic from above is reverted now. A single item is used as input, aka <code class="highlighter-rouge">count_item</code>, and the result <code class="highlighter-rouge">counts</code> is a list again. In order to compute pi from this, the following operations should be restricted to <code class="highlighter-rouge">rank=0</code> in order to minimize redundant operations:</p>

<div class="python highlighter-rouge"><pre class="highlight"><code>if rank == 0:
    my_pi = 4.0 * sum(counts) / sum(partitions)
</code></pre>
</div>

<p>And that’s it. Now, Lola can submit her first MPI job.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">$ bsub -n 48 -o mpi_numpi.out -e mpi_numpi.err mpirun python3 mpi_numpi.py 1000000000</code></pre></figure>

<p>The output file <code class="highlighter-rouge">mpi_numpi.out</code> yields the following lines:</p>

<div class="output highlighter-rouge"><pre class="highlight"><code>[     mpi version] required memory 11444.092 MB
[using  48 cores ] pi is 3.141679 from 1000000000 samples

real    0m6.368s
user    0m45.763s
sys     0m6.681s
</code></pre>
</div>

<p>Note here, that we are now free to scale this application to hundreds of core if we wanted to. We are only restricted by the size of our compute cluster. Before finishing the day, Lola looks at the runtime that here MPI job consumed. <code class="highlighter-rouge">6.4</code> seconds for a job that ran on twice as much cores as here parallel implementation. That is quite an achievement of the day!</p>

<blockquote class="challenge">
  <h2 id="use-the-batch-system">Use the batch system!</h2>

  <p>Launch the serial and parallel version of the pi_estimate using the batch system.</p>

</blockquote>

<blockquote class="challenge">
  <h2 id="dont-stress-the-network">Don’t Stress the Network</h2>

  <p>The MPI implementation given above transmits only the pi estimate per rank to the main program. Rewrite the program so that each rank generates the random numbers and sends them back to rank 0.</p>

  <p>Submit the job and look at the time it took. What do you observe? Why did the runtime change?</p>
</blockquote>


<blockquote class="keypoints">
  <h2>Key Points</h2>
  <ul>
    
  </ul>
</blockquote>


<div class="row">
  <div class="col-md-1">
    <h3>
      
      <a href="../03-01-parallel-estimate-of-pi/"><span class="glyphicon glyphicon-menu-left" aria-hidden="true"></span><span class="sr-only">previous episode</span></a>
      
    </h3>
  </div>
  <div class="col-md-10">
    
  </div>
  <div class="col-md-1">
    <h3>
      
      <a href="../03-03-mapreduce-for-pi/"><span class="glyphicon glyphicon-menu-right" aria-hidden="true"></span><span class="sr-only">next episode</span></a>
      
    </h3>
  </div>
</div>


      
      
<footer>
  <div class="row">
    <div class="col-md-6" align="left">
      <h4>
	Copyright &copy; 2017
	
	<a href="https://software-carpentry.org">Software Carpentry Foundation</a>
	
      </h4>
    </div>
    <div class="col-md-6" align="right">
      <h4>
	<a href="/">Source</a>
	/
	<a href="/blob/gh-pages/CONTRIBUTING.md">Contributing</a>
	/
	<a href="/blob/gh-pages/CITATION">Cite</a>
	/
	<a href="">Contact</a>
      </h4>
    </div>
  </div>
</footer>

      
    </div>
    
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/bootstrap.min.js"></script>
<script src="../assets/js/lesson.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-37305346-2', 'auto');
  ga('send', 'pageview');
</script>

  </body>
</html>
